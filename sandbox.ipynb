{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_question_generator_chain' from 'langchain.chains' (/Users/alexlakiza/py/magnifico-assistente-intervista/venv/lib/python3.10/site-packages/langchain/chains/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 15\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConversationChain, ConversationalRetrievalChain, LLMChain\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mquestion_answering\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_qa_chain\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_question_generator_chain\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_community\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membeddings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GigaChatEmbeddings\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_openai\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OpenAIEmbeddings\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'load_question_generator_chain' from 'langchain.chains' (/Users/alexlakiza/py/magnifico-assistente-intervista/venv/lib/python3.10/site-packages/langchain/chains/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import operator\n",
    "import functools\n",
    "\n",
    "# from langchain.chat_models.gigachat import GigaChat\n",
    "from langchain_community.chat_models.gigachat import GigaChat\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, \\\n",
    "    ConversationSummaryMemory, VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain, ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "# from langchain.chains import load_question_generator_chain\n",
    "\n",
    "from langchain_community.embeddings import GigaChatEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.tools import tool\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load environment variables\n",
    "load_dotenv('.env')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# class SimpleChatBot:\n",
    "#     def __init__(self, llm, memory_type, window_size=None):\n",
    "#         if memory_type == 'window':\n",
    "#             assert window_size is not None\n",
    "#             assert type(window_size) == int\n",
    "#\n",
    "#         self.memory_types = {\n",
    "#             'simple': ConversationBufferMemory(),\n",
    "#             'summary': ConversationSummaryMemory(llm=llm),\n",
    "#             'window': ConversationBufferWindowMemory(k=window_size)}\n",
    "#\n",
    "#         self.memory = self.memory_types[memory_type]\n",
    "#\n",
    "#\n",
    "#         self.conversation = ConversationChain(\n",
    "#             llm=llm,\n",
    "#             verbose=True,\n",
    "#             memory=self.memory\n",
    "#         )\n",
    "#\n",
    "#\n",
    "#     def _respond(self, user_input):\n",
    "#         return self.conversation.invoke(user_input)\n",
    "#\n",
    "#     def print_memory(self):\n",
    "#         print(self.conversation.memory.load_memory_variables({})['history'])\n",
    "#\n",
    "#     def run(self):\n",
    "#         print(\"This is a simple chat bot:\")\n",
    "#         #TODO\n",
    "#         while True:\n",
    "#             user_input = input(\"User: \")\n",
    "#             if user_input == \"\":\n",
    "#                 break\n",
    "#             print(f\"User: {user_input}\")\n",
    "#             try:\n",
    "#                 model_response = self._respond(user_input=user_input)\n",
    "#                 print(f\"AI: {model_response['response']}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error: {str(e)}\")\n",
    "#         time.sleep(0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "giga_key = os.environ.get(\"SB_AUTH_DATA\")\n",
    "giga = GigaChat(credentials=giga_key, model=\"GigaChat\", timeout=30, verify_ssl_certs=False)\n",
    "# chat = SimpleChatBot(giga, memory_type='summary', window_size=2)\n",
    "# chat.run()\n",
    "# print('=='*10)\n",
    "# chat.print_memory()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 2/2 [00:00<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "urls = [\"https://uproger.com/100-voprosov-c-sobesov-v-data-science-i-ml\", \"https://kalashnikof.com/blog/voprosy-dlya-sobesedovaniya-frontend-react/\"]\n",
    "loader = AsyncHtmlLoader(urls)\n",
    "docs = loader.load()\n",
    "\n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Step 1: Create or Load Knowledge Base\n",
    "# This section assumes you've preprocessed and indexed interview questions from multiple websites.\n",
    "# You can use FAISS or other vector databases for RAG.\n",
    "\n",
    "def create_knowledge_base(documents):\n",
    "    \"\"\"Creates a vectorstore for the knowledge base.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"DeepPavlov/rubert-base-cased-sentence\")\n",
    "    # embeddings = GigaChatEmbeddings(verify_ssl_certs=False, scope=\"GIGACHAT_API_PERS\")\n",
    "    vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "def get_retrieval_chain(vectorstore):\n",
    "    \"\"\"Creates a conversational retrieval chain.\"\"\"\n",
    "\n",
    "    # 1. Define an LLM\n",
    "    # llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "    # 2. Define a chain for combining documents\n",
    "    combine_docs_prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=(\n",
    "            \"You are a helpful assistant. Use the following context to answer the question.\\n\\n\"\n",
    "            \"Context:\\n{context}\\n\\n\"\n",
    "            \"Question:\\n{question}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        ),\n",
    "    )\n",
    "    combine_docs_chain = load_qa_chain(giga, chain_type=\"stuff\", prompt=combine_docs_prompt)\n",
    "\n",
    "    # 3. Define a question generator chain\n",
    "    question_generator_prompt = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"question\"],\n",
    "        template=(\n",
    "            \"Given the following conversation history and a follow-up question, rewrite the follow-up \"\n",
    "            \"question to make it a standalone question.\\n\\n\"\n",
    "            \"Conversation history:\\n{chat_history}\\n\\n\"\n",
    "            \"Follow-up question:\\n{question}\\n\\n\"\n",
    "            \"Standalone question:\"\n",
    "        ),\n",
    "    )\n",
    "    question_generator_chain = LLMChain(llm=giga, prompt=question_generator_prompt)\n",
    "\n",
    "    # 4. Add memory for chat history\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "    # 5. Build the ConversationalRetrievalChain\n",
    "    retrieval_chain = ConversationalRetrievalChain(\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "        question_generator=question_generator_chain,\n",
    "        combine_docs_chain=combine_docs_chain,\n",
    "        memory=memory,\n",
    "    )\n",
    "\n",
    "    return retrieval_chain\n",
    "\n",
    "\n",
    "# Step 3: Generate Topics and Questions from Vacancy Description\n",
    "# Define a prompt tem\n",
    "# plate for extracting interview topics and questions.\n",
    "TOPICS_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    input_variables=[\"vacancy\"],\n",
    "    template=(\n",
    "        \"Given the following job description: \\n\"\n",
    "        \"{vacancy}\\n\"\n",
    "        \"1. Extract a list of relevant topics that the candidate should prepare for.\\n\"\n",
    "        \"2. Provide sample interview questions and suggested answers for each topic.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "def generate_topics_and_questions(vacancy_description, retrieval_chain=None):\n",
    "    \"\"\"Generates interview topics, questions, and answers by combining LLM knowledge and retrieved knowledge base.\"\"\"\n",
    "    retrieved_knowledge = \"\"\n",
    "\n",
    "    # Step 1: Retrieve knowledge from the knowledge base if the chain is provided\n",
    "    if retrieval_chain:\n",
    "        knowledge_results = retrieval_chain({\"question\": vacancy_description})\n",
    "        retrieved_knowledge = knowledge_results['answer']\n",
    "\n",
    "    # Step 2: Combine both sources of knowledge into a single prompt for the LLM\n",
    "    combined_prompt = (\n",
    "        \"You are an IT interview assistant helping a candidate prepare for a job interview.\\n\\n\"\n",
    "        \"Job Description:\\n\"\n",
    "        f\"{vacancy_description}\\n\\n\"\n",
    "        \"Additional Knowledge from the database:\\n\"\n",
    "        f\"{retrieved_knowledge}\\n\\n\"\n",
    "        \"Based on the job description and the additional knowledge:\\n\"\n",
    "        \"1. Extract a list of relevant topics the candidate should prepare for.\\n\"\n",
    "        \"2. Provide sample interview questions for each topic.\\n\"\n",
    "        \"3. Provide concise suggested answers for each question.\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Generate the response using the LLM\n",
    "    response = giga.invoke(combined_prompt).content\n",
    "    return response\n",
    "\n",
    "\n",
    "# Step 4: URL Parsing Functionality\n",
    "# Load content from a given URL.\n",
    "\n",
    "def process_url(url):\n",
    "    \"\"\"Fetches and preprocesses content from a URL.\"\"\"\n",
    "    loader = WebBaseLoader(url)\n",
    "    documents = loader.load()\n",
    "    return documents[0].page_content if documents else \"\"\n",
    "\n",
    "# Main Chatbot Functionality\n",
    "# Example: Case 1 (Description Input) and Case 2 (URL Input)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name DeepPavlov/rubert-base-cased-sentence. Creating a new one with mean pooling.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the IT Interview Assistant Bot!\n",
      "Choose an option:\n",
      "1. Provide a job description\n",
      "2. Provide a URL to a job posting\n",
      "\n",
      "Topics and Questions:\n",
      "\n",
      "### Подготовка к интервью: список тем и вопросы\n",
      "\n",
      "1. **Анализ данных с использованием Python**\n",
      "   - Какие библиотеки для анализа данных вы чаще всего используете? Почему именно они?\n",
      "   - Как вы решаете проблему нехватки данных для обучения модели?\n",
      "   - Расскажите о вашем опыте использования библиотек Pandas и NumPy для обработки больших объемов данных.\n",
      "\n",
      "2. **SQL**\n",
      "   - Опишите, как вы использовали SQL для выполнения сложных запросов и агрегации данных.\n",
      "   - Приведите примеры, когда SQL был полезен для оптимизации производительности приложения.\n",
      "\n",
      "3. **Машинное обучение и статистический анализ**\n",
      "   - Что вы понимаете под термином \"переобучение\" в контексте машинного обучения? Как вы его избегаете?\n",
      "   - Какую роль играет кросс-валидация в оценке качества модели? Приведите пример.\n",
      "   - В чем разница между линейной регрессией и логистической регрессией? Когда вы бы применили одну вместо другой?\n",
      "\n",
      "4. **Статистический анализ**\n",
      "   - Объясните, зачем нужна проверка гипотез в статистическом анализе. Приведите пример такой проверки.\n",
      "   - Почему важно проводить A/B тестирование? Приведите пример такого тестирования.\n",
      "   - Как вы применяете теорию вероятностей в своей работе? Приведите пример.\n",
      "\n",
      "5. **Обработка данных и визуализация**\n",
      "   - Какой инструмент для визуализации данных вы предпочитаете и почему?\n",
      "   - Предложите подход к визуализации данных для анализа клиентского поведения.\n",
      "   - Как вы выбираете параметры визуализации данных в зависимости от цели проекта?\n",
      "\n",
      "6. **Общий обзор знаний в Data Science**\n",
      "   - Назовите основные этапы процесса разработки ML-модели.\n",
      "   - Как вы поддерживаете актуальность ваших знаний в области Data Science?\n",
      "   - Чем отличается работа Data Scientist от работы обычного программиста?\n",
      "\n",
      "### Пример вопросов и ответов\n",
      "\n",
      "**Вопрос:** Какие библиотеки для анализа данных вы чаще всего используете? Почему именно они?\n",
      "\n",
      "**Ответ:** Я обычно использую библиотеки `pandas` и `numpy`, так как они позволяют эффективно обрабатывать большие объемы данных и выполнять различные операции с массивами данных. Эти библиотеки предоставляют мощные инструменты для манипуляции данными, такие как Series, DataFrame, группировка, агрегирование и преобразование данных. Кроме того, я часто использую `matplotlib` для создания визуализаций, так как он предлагает гибкие возможности для представления данных в удобной форме.\n",
      "\n",
      "**Вопрос:** Как вы решаете проблему нехватки данных для обучения модели?\n",
      "\n",
      "**Ответ:** Для решения проблемы нехватки данных я обычно прибегаю к следующим стратегиям:\n",
      "1. **Сбор дополнительных данных**: Если возможно, я собираю дополнительные данные из различных источников, например, через API или собственные эксперименты.\n",
      "2. **Применение методов синтетической генерации данных**: Использую методы синтеза данных, такие как генеративно-состязательные сети (GANs), чтобы создать новые образцы данных, соответствующие существующим данным.\n",
      "3. **Методы ансамблевого обучения**: Создаю ансамбли моделей, комбинируя результаты нескольких моделей, обученных на разных выборках данных.\n",
      "4. **Выборочное обучение**: Применяю техники регуляризации, такие как L1/L2 нормализация или раннюю остановку, чтобы уменьшить переобучение на имеющихся данных.\n",
      "\n",
      "**Вопрос:** Что вы понимаете под термином \"переобучение\" в контексте машинного обучения? Как вы его избегаете?\n",
      "\n",
      "**Ответ:** Переобучение — это ситуация, когда модель чрезмерно адаптируется к обучающим данным, что приводит к низкой точности на новых данных. Избежать переобучения можно следующими способами:\n",
      "1. **Регуляризация**: Использование методов регуляризации, таких как L1/L2 нормализация весов, dropout или раннее прекращение обучения.\n",
      "2. **Кросс-валидация**: Разделение данных на несколько блоков и использование каждой части для проверки модели, что позволяет получить более точные оценки производительности.\n",
      "3. **Добавление шума**: Добавление небольшого количества случайного шума в обучающие данные может улучшить обобщенную способность модели.\n",
      "4. **Ансамблирование моделей**: Комбинация результатов нескольких моделей для повышения точности.\n",
      "\n",
      "**Вопрос:** Какую роль играет кросс-валидация в оценке качества модели? Приведите пример.\n",
      "\n",
      "**Ответ:** Кросс-валидация используется для оценки обобщенной производительности модели, минимизируя риск переобучения. Например, если у нас есть набор данных из 100 наблюдений, мы можем разделить эти данные на пять блоков (или k-блоков). Затем мы последовательно обучаем модель на четырех блоках и проверяем ее на пятом блоке. Этот процесс повторяется для всех пяти блоков, и в конце мы получаем среднее значение этих пяти оценок. Это позволяет нам получить более реалистичную оценку того, как модель будет вести себя на новых данных.\n",
      "\n",
      "**Вопрос:** В чем разница между линейной регрессией и логистической регрессией? Когда вы бы применили одну вместо другой?\n",
      "\n",
      "**Ответ:** Линейная регрессия применяется для предсказания непрерывных переменных, тогда как логистическая регрессия используется для классификации, где результат является категориальным. Логистическая регрессия подходит для задач с двумя возможными исходами (например, да/нет), в то время как линейная регрессия предназначена для предсказания количественных значений. Если нужно классифицировать данные, я бы выбрал логистическую регрессию; если же требуется предсказать числовые значения, я бы использовал линейную регрессию.\n",
      "\n",
      "---\n",
      "\n",
      "Это лишь некоторые примеры вопросов и ответов, которые могут быть использованы в интервью. Важно помнить, что каждый кандидат уникален, и интервьюер может задавать другие вопросы или акцентировать внимание на других аспектах подготовки.\n"
     ]
    }
   ],
   "source": [
    "documents = docs_transformed  # Replace with your document loading logic\n",
    "vectorstore = create_knowledge_base(documents)\n",
    "retrieval_chain = get_retrieval_chain(vectorstore)\n",
    "\n",
    "print(\"Welcome to the IT Interview Assistant Bot!\")\n",
    "print(\"Choose an option:\")\n",
    "print(\"1. Provide a job description\")\n",
    "print(\"2. Provide a URL to a job posting\")\n",
    "choice = input(\"Enter your choice: \")\n",
    "\n",
    "if choice == \"1\":\n",
    "    vacancy_description = input(\"Enter the job description: \")\n",
    "    output = generate_topics_and_questions(vacancy_description, retrieval_chain)\n",
    "    print(\"\\nTopics and Questions:\\n\")\n",
    "    print(output)\n",
    "\n",
    "elif choice == \"2\":\n",
    "    url = input(\"Enter the URL of the job posting: \")\n",
    "    job_description = process_url(url)\n",
    "    if job_description:\n",
    "        output = generate_topics_and_questions(job_description, retrieval_chain)\n",
    "        print(\"\\nTopics and Questions:\\n\")\n",
    "        print(output)\n",
    "    else:\n",
    "        print(\"Failed to retrieve content from the URL.\")\n",
    "\n",
    "else:\n",
    "    print(\"Invalid choice. Please try again.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
