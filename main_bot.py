import asyncio
import logging
import os
import sys

from aiogram import Bot, Dispatcher, html, types, F
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.filters import Command
from dotenv import load_dotenv
from langchain.chains import ConversationChain
from langchain.chains import ConversationalRetrievalChain, LLMChain
from langchain.chains.question_answering import load_qa_chain
from langchain.document_loaders import WebBaseLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.memory import ConversationBufferMemory, \
    ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_community.chat_models.gigachat import GigaChat
from langchain_community.document_loaders import AsyncHtmlLoader
from langchain_community.document_transformers import Html2TextTransformer

load_dotenv()
dp = Dispatcher()

BOT_TOKEN = os.getenv('BOT_TOKEN')
giga_key = os.environ.get("SB_AUTH_DATA")
giga = GigaChat(credentials=giga_key, model="GigaChat", timeout=30,
                verify_ssl_certs=False, max_tokens=900)


giga_chain = ConversationChain(
        llm=giga,
        verbose=True,
        memory=ConversationBufferWindowMemory(k=2)
    )


def get_rag_database(list_of_urls):
    loader = AsyncHtmlLoader(list_of_urls)
    docs = loader.load()

    html2text = Html2TextTransformer()
    docs_transformed = html2text.transform_documents(docs)

    return docs_transformed


def create_knowledge_base(documents):
    """Creates a vectorstore for the knowledge base."""
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300,
                                                   chunk_overlap=50)
    split_docs = text_splitter.split_documents(documents)
    embeddings = HuggingFaceEmbeddings(
        model_name="DeepPavlov/rubert-base-cased-sentence")
    vectorstore = FAISS.from_documents(split_docs, embeddings)
    return vectorstore


def get_retrieval_chain(vectorstore):
    """Creates a conversational retrieval chain."""

    combine_docs_prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "Ð¢Ñ‹ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¹ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ¸ Ðº "
            "ÑÐ¾Ð±ÐµÑÐµÐ´Ð¾Ð²Ð°Ð½Ð¸ÑŽ Ð² IT-ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸ÑŽ. "
            "Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¿Ð¾Ð´Ð±ÐµÑ€Ð¸ ÑÐ¿Ð¸ÑÐ¾Ðº Ñ‚ÐµÐ¼ Ð´Ð»Ñ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸.\n\n"
            "ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:\n{context}\n\n"
            "Ð’Ð°ÐºÐ°Ð½ÑÐ¸Ñ:\n{question}\n\n"
            "ÐžÑ‚Ð²ÐµÑ‚:"
        ),
    )
    combine_docs_chain = load_qa_chain(giga, chain_type="stuff",
                                       prompt=combine_docs_prompt)

    # question_generator_prompt = PromptTemplate(
    #     input_variables=["chat_history", "question"],
    #     template=(
    #         "Ð£Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ñ‡Ð°Ñ‚Ð° Ð¸ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸ Ð² Ð²Ð¾Ð¿Ñ€Ð¾ÑÐµ, "
    #         "Ð¿ÐµÑ€ÐµÐ¿Ð¸ÑˆÐ¸ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸ Ñ‚Ð°Ðº, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ñ‚ÑŒÑÑ "
    #         "Ðº ÑÐ¾Ð±ÐµÑÐµÐ´Ð¾Ð²Ð°Ð½Ð¸ÑŽ Ð½Ð° ÑÑ‚Ñƒ Ð²Ð°ÐºÐ°Ð½ÑÐ¸ÑŽ.\n\n"
    #         "Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ñ‡Ð°Ñ‚Ð°:{chat_history}\n\n"
    #         "ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸:{question}"
    #     ),
    # )
    # question_generator_chain = LLMChain(llm=giga,
    #                                     prompt=question_generator_prompt)

    memory = ConversationBufferMemory(memory_key="chat_history",
                                      return_messages=True)

    conv_chain = ConversationalRetrievalChain(
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        question_generator=giga_chain,
        combine_docs_chain=combine_docs_chain,
        memory=memory,
    )

    return conv_chain


def generate_topics_and_questions(vacancy_description,
                                  retrieval_chain=None):
    """Generates interview topics, questions, and answers
    by combining LLM knowledge and retrieved knowledge base."""
    retrieved_knowledge = ""

    # Step 1: Retrieve knowledge from the knowledge base if
    # the chain is provided
    if retrieval_chain:
        try:
            knowledge_results = retrieval_chain({"question":
                                                     vacancy_description})
            retrieved_knowledge = knowledge_results['answer']
        except ValueError:
            retrieved_knowledge = ""

    # Step 2: Combine both sources of knowledge into a single prompt
    # for the LLM
    combined_prompt = (
        "Ð¢Ñ‹ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑˆÑŒ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ñ‚ÑŒÑÑ IT-ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚Ñƒ Ðº ÑÐ¾Ð±ÐµÑÐµÐ´Ð¾Ð²Ð°Ð½Ð¸ÑŽ Ð½Ð° "
        "Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ.\n\n"
        "ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸:\n"
        f"{vacancy_description}\n\n"
        "Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð·Ð½Ð°Ð½Ð¸Ñ Ð´Ð»Ñ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ¸ Ð¸Ð· Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹:\n"
        f"{retrieved_knowledge}\n\n"
        "ÐžÐ¿Ð¸Ñ€Ð°ÑÑÑŒ Ð½Ð° Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸ Ð¸ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð·Ð½Ð°Ð½Ð¸Ñ:\n"
        "1. ÐŸÑ€Ð¸Ð²ÐµÐ´Ð¸ ÑÐ¿Ð¸ÑÐ¾Ðº 3 Ñ‚ÐµÐ¼ Ð´Ð»Ñ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ¸.\n"
        "2. ÐŸÑ€Ð¸Ð²ÐµÐ´Ð¸ Ð¿Ð¾ 3 Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° Ð½Ð° ÐºÐ°Ð¶Ð´ÑƒÑŽ Ñ‚ÐµÐ¼Ñƒ.\n"
        "3. ÐŸÑ€Ð¸Ð²ÐµÐ´Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ñ… Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹.\n\n"
        "Ð”Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ñ‚Ð²Ð¾ÐµÐ³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð² 3600 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð². "
        "ÐÐµ Ð¿Ð¸ÑˆÐ¸ Ð² ÑÐ²Ð¾ÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚Ðµ Ð½Ð¸Ñ‡ÐµÐ³Ð¾, ÐºÑ€Ð¾Ð¼Ðµ "
        "Ð½Ð°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ñ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸, Ñ‚ÐµÐ¼, Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð². ÐÐµ Ð·Ð°Ð´Ð°Ð²Ð°Ð¹ "
        "Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ð½Ðµ Ð¿Ð¸ÑˆÐ¸ Ð²Ñ‹Ð²Ð¾Ð´Ñ‹ Ð¸Ð»Ð¸ Ð¸Ñ‚Ð¾Ð³Ð¸ Ð² ÑÐ²Ð¾ÐµÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¸"
    )

    print(f"{combined_prompt=}")

    # Step 3: Generate the response using the LLM
    response = giga_chain.invoke(combined_prompt)
    return response['response']


def process_url(url):
    """Fetches and preprocesses content from a URL."""
    loader = WebBaseLoader(url)
    documents = loader.load()
    return documents[0].page_content if documents else ""


# ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ° Telegram-Ð±Ð¾Ñ‚Ð°
@dp.message(Command("start"))
async def command_start_handler(message: types.Message) -> None:
    await message.answer(f"ÐŸÑ€Ð¸Ð²ÐµÑ‚, {html.bold(message.from_user.full_name)}! "
                         f"Ð¯ Ð²Ð°Ñˆ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð¿Ð¾ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐµ Ðº IT ÑÐ¾Ð±ÐµÑÐµÐ´Ð¾Ð²Ð°Ð½Ð¸ÑÐ¼. "
                         "ÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸, Ð¸ Ñ Ð²Ð°Ð¼ Ð¿Ð¾Ð¼Ð¾Ð³Ñƒ!")


@dp.message(Command("new"))
async def command_new_handler(message: types.Message) -> None:
    await message.answer("ÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸, Ð¸ Ñ Ð²Ð°Ð¼ Ð¿Ð¾Ð¼Ð¾Ð³Ñƒ!")


@dp.message(F.text.lower() == "ÐµÑ‰Ñ‘ Ñ‚ÐµÐ¼Ñ‹ Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ ðŸš€")
async def more_topics(message: types.Message):
    try:
        response = giga_chain.invoke("User: "
                                     "Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐµ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸, "
                                     "Ð½Ð°Ð¿Ð¸ÑˆÐ¸ ÑÐ¿Ð¸ÑÐ¾Ðº Ð½Ð¾Ð²Ñ‹Ñ… "
                                     "Ñ‚ÐµÐ¼, Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð½Ð° ÑÑ‚Ð¸ Ñ‚ÐµÐ¼Ñ‹ Ð¸ "
                                     "(Ð²Ð°Ð¶Ð½Ð¾) Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ Ð½Ð° "
                                     "Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹."
                                     "Ð’Ð°Ð¶Ð½Ð¾, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð´Ð»Ñ ÑÑ‚Ð¾Ð¹ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸ "
                                     "Ð½Ðµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÐ»Ð¸ÑÑŒ. "
                                     "ÐÐµ Ð·Ð°Ð´Ð°Ð²Ð°Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð½Ñ‹Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹. "
                                     "ÐÐµ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°Ð¹ Ñ‚ÐµÐ¼Ñ‹ Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð´Ð»Ñ "
                                     "Ð½Ð¾Ð²Ð¾Ð¹ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ñ€Ð¾ "
                                     "ÑÑ‚Ð°Ñ€ÑƒÑŽ Ð²Ð°ÐºÐ°Ð½ÑÐ¸ÑŽ. ÐžÑ‚ÐºÐ°Ð· Ð½Ðµ Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°ÐµÑ‚ÑÑ")
        print(response)

        await message.answer(response['response'],
                             reply_markup=keyboard)
    except Exception as e:
        await message.answer(f'ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° {e}')


kb = [
    [
        types.KeyboardButton(text="Ð•Ñ‰Ñ‘ Ñ‚ÐµÐ¼Ñ‹ Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ ðŸš€"),
        types.KeyboardButton(text="ÐÐ¾Ð²Ð°Ñ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ñ ðŸ’¼")
    ],
]

keyboard = types.ReplyKeyboardMarkup(
    keyboard=kb,
    resize_keyboard=True,
    input_field_placeholder="Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð´Ð°Ð»ÑŒÐ½ÐµÐ¹ÑˆÐ¸Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ"
)


@dp.message(F.text.lower() == "Ð½Ð¾Ð²Ð°Ñ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ñ ðŸ’¼")
async def new_vacancy(message: types.Message):
    await message.answer("ÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸, Ð¸ Ñ Ð²Ð°Ð¼ Ð¿Ð¾Ð¼Ð¾Ð³Ñƒ!",
                         reply_markup=types.ReplyKeyboardRemove())


@dp.message()
async def handle_message(message: types.Message):
    user_query = f"User: {message.text}"

    try:
        response = generate_topics_and_questions(user_query, retrieval_chain)
        print(f"===\n{response=}\n===")

        await message.answer(response,
                             reply_markup=keyboard)
    except Exception as e:
        await message.answer(f'ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° {e}')


async def main() -> None:
    bot = Bot(token=BOT_TOKEN,
              default=DefaultBotProperties(parse_mode=ParseMode.HTML))

    # And the run events dispatching
    await dp.start_polling(bot)


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO, stream=sys.stdout)

    urls = ["https://uproger.com/100-voprosov-c-sobesov-v-data-science-i-ml",
            "https://kalashnikof.com/blog/voprosy-dlya-"
            "sobesedovaniya-frontend-react/"]
    rag_database = get_rag_database(urls)
    vectorstore = create_knowledge_base(rag_database)
    print('5')
    retrieval_chain = get_retrieval_chain(vectorstore)

    asyncio.run(main())
